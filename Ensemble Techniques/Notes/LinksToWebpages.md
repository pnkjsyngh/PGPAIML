### Week 1
- [Introduction to Bagging [An overview of ensemble learning, introduction and application of bagging, hyperparameters explanation and implementation of BaggingClassifier]](https://blog.paperspace.com/bagging-ensemble-methods/)
- [Introduction to Random Forest (Articles explaining what is a random forest and steps to build it, advantages and disadvantages of random forests)](https://blog.paperspace.com/random-forests/)
- [Implementation of Random Forest (Article that explains random forest along with an example of implementation using sklearn)](https://www.mygreatlearning.com/blog/random-forest-algorithm/)
  
### Week 2
- [Introduction to Boosting - An article which explains what is boosting and what are different types of boosting algorithms then explains AdaBoost in detail - concepts as well as implementation in Python with the advantages and disadvantages of AdaBoost](https://blog.paperspace.com/adaboost-optimizer/)
- [Introduction to Gradient Boosting - An article which explains gradient boosting from scratch - intuition, implementation and mathematics behind it and good visual explanations](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)
- [Implementation of Gradient Boosting with Sklearn](https://stackabuse.com/gradient-boosting-classifiers-in-python-with-scikit-learn/)
- [Beginners guide to XGBoost](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)
- [Original paper of XGBoost - This is the original paper on XGBoost. It explains the mathematics behind the algorithm, salient features like Weighted Quantile Sketch, Sparsity-aware Split Finding, etc](https://arxiv.org/pdf/1603.02754.pdf)
